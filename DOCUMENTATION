TRANSPILATION

(Note: As of v4.9.1, the -t flag is deprecated in favor of Smart Output Detection. Yori automatically detects if you want source code or binary based on the output filename extension).

To output source code (Transpilation):

yori file.py -o mysrc.c -local      <== Outputs .c text file because extension matches target language
yori file.py -o mysrc.cpp -local    <== Outputs .cpp text file

To output binary (Compilation):

yori file.cpp -o myapp.exe -local   <== Outputs compiled binary

MULTIPLE FILE LINKING

Yori supports compilation between multiple source files. Yori uses the backend AI to understand the intent of the source code logic, it then transpiles the code to the target language and compiles (if applies) with the corresponding toolchain (if found).

yori fileA.c fileB.cpp fileC.hpp fileD.py fileE.js -o myapp.exe -cloud

UNIVERSAL IMPORTS

Yori files (such as .yori .acn or any custom extension) support universal imports using IMPORT: keyword. This recursively resolves dependencies before sending context to the AI. Example:

IMPORT: a.cpp COM= for function1()
IMPORT: b.py COM= for function2()
IMPORT: c.js COM= for function3()

INT x = 1
//program logic
function1(x)
function2(x)
function3(x)


UNIVERSAL PREPROCESSOR DIRECTIVES

Yori uses a comment system to give the backend AI indications on how to handle the code.
// !!! (literal, do what the code says, no improvise, no smart solutions, be blindly obedient)
// >>> (relaxed, this tells the compiler it has creative freedom to optimize and wiggle room to interpet intent)
// ??? (user unsure, this tells the compiler the user is not sure which state of the above to use, the AI proceeds with caution)

This directives can be used not only in Yori files, but they are extensible to any source code file. Since Yori accepts sources form any language, users can use those directives in their programs to modulate the compiler's AI behavior.

# !!! implement this algorithm as is
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):
        for j in range(0, n-i-1):
            if arr[j] > arr[j+1]:
                arr[j], arr[j+1] = arr[j+1], arr[j]
    return arr


YORI TECHNICAL REFERENCE (v4.9.1)

CLI COMMANDS & USAGE

Basic Syntax:
yori <source_files> [options]

Management Commands:

yori config <key> <value>: Updates config.json manually.

yori config model-local: Interactive menu to scan and select installed Ollama models.

yori get-key: Opens the browser to generate a Google API Key (legacy default).

yori --init: Generates a project template (hello.yori and config.json).

yori --clean: Removes temporary build files (temp_build.*) and the build cache (.yori_build.cache).

FLAGS & OPTIONS

Flag

Description

-o <file>

Specify output filename. Determines Binary vs Source mode automatically.

-u, --update

Update Mode. Reads the existing output file and asks AI to modify it instead of generating from scratch. Useful for iterative changes.

-cloud

Use the Cloud Provider defined in config.json.

-local

Use the Local Provider (default: Ollama) defined in config.json.

-dry-run

Prints the preprocessed context (imports resolved) to stdout without calling AI or compiling.

-verbose

Enables detailed logging to console (Raw API responses, debug info).

--version

Displays current Yori version.

--help, -h

Shows the help menu.

Language Overrides:
You can force a specific target language context using flags:
-cpp, -c, -rust, -go, -py, -js, -ts, -cs, -java, -php, -rb, -lua, -pl, -sh, -swift, -kt, -scala, -hs, -jl, -dart, -zig, -nim, -r.

CONFIGURATION (config.json)

Yori uses a config.json file in the working directory. It creates one with --init if missing.

Structure:

{
    "local": {
        "model_id": "qwen2.5-coder:3b",
        "api_url": "http://localhost:11434/api/generate"
    },
    "cloud": {
        "protocol": "openai",         // Options: 'google', 'openai', 'ollama'
        "api_key": "sk-...",
        "model_id": "gpt-4o",
        "api_url": "[https://api.openai.com/v1/chat/completions](https://api.openai.com/v1/chat/completions)"
    },
    "toolchains": {                   // Override default build commands
        "cpp": {
            "build_cmd": "clang++ -O3"
        }
    }
}


Configuration Keys (for yori config command):

api-key: Sets the cloud API Key.

cloud-protocol: Sets the API format (openai for Groq/DeepSeek/GPT, google for Gemini, ollama for remote local).

model-cloud: Model ID for cloud requests.

url-cloud: Full API Endpoint for cloud.

model-local: Model ID for local requests.

url-local: Full API Endpoint for local.

TECHNICAL ARCHITECTURE

Ingestion & Import Resolution:
Yori scans source files for IMPORT: filename. It recursively resolves these paths to build a single "Context" string, detecting cyclic dependencies.

Pre-Flight Dependency Check:
Before calling the AI, Yori extracts potential #include or missing import warnings. It creates a dummy temporary file and tries to compile it (flag -c) to verify if headers/libraries exist on the host system. If this fails, it aborts to save AI tokens/time.

Caching System:
Yori hashes the input context + language + model + mode. If a .yori_build.cache exists and matches the hash, it skips generation and compilation, returning the previous build instantly.

AI Generation & Error Memory:
Yori sends the prompt to the provider. If the compiled code fails, Yori feeds the compiler error back to the AI (Error Memory) and requests a fix. It retries up to 15 times.

Smart Output:

If -o output.cpp (extension matches source), Yori saves the generated code.

If -o output.exe (extension differs), Yori saves the compiled binary and cleans up the source.

SUPPORTED PROTOCOLS

google: Uses the contents/parts/text JSON structure. (Legacy Gemini API).

openai: Uses the standard messages/role/content structure. Compatible with OpenAI, Groq, DeepSeek, OpenRouter, etc.

ollama: Uses the prompt/response structure optimized for local inference.